{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 1,
=======
   "execution_count": 53,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "557585c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 2,
=======
   "execution_count": 54,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "7ebd1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 제곱 오차\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 3,
=======
   "execution_count": 55,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "9afd4dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 4,
=======
   "execution_count": 56,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "4397e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "# log(0)은 inf기 때문에 임의의 작은 값 delta를 더함"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 5,
=======
   "execution_count": 57,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "77b60cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 6,
=======
   "execution_count": 58,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "f04174e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 7,
=======
   "execution_count": 59,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "462a5161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  60000\n",
<<<<<<< HEAD:ch4/chapter4.ipynb
      "[51852  7468 25842  9581 47999 56952 57235 50168  7848   710]\n",
=======
      "[23110 28521  8085  7548 39024 15274 32487  4176 47202 15359]\n",
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "print(\"train_size: \", train_size)\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(len(x_batch))\n",
    "print(len(t_batch))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 8,
=======
   "execution_count": 60,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "da2b6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치용 교차 엔트로피 구현\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+ 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 9,
=======
   "execution_count": 61,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "6297e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 핫 인코딩이 아닌 숫자 레이블로 주어졌을 때 엔트로피 오차\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64c874",
   "metadata": {},
   "source": [
    "### 수지해석"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 10,
=======
   "execution_count": 62,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "8e0e46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 11,
=======
   "execution_count": 63,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "d83a2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 12,
=======
   "execution_count": 64,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "d7d4b96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 13,
=======
   "execution_count": 65,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "fa104e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 14,
=======
   "execution_count": 66,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "e456b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 15,
=======
   "execution_count": 67,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "ce97269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 16,
=======
   "execution_count": 68,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "bb421ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
<<<<<<< HEAD:ch4/chapter4.ipynb
     "execution_count": 16,
=======
     "execution_count": 68,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 17,
=======
   "execution_count": 69,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "49be7243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
<<<<<<< HEAD:ch4/chapter4.ipynb
     "execution_count": 17,
=======
     "execution_count": 69,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 큰 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x =init_x, lr=10.0, step_num =100)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 18,
=======
   "execution_count": 70,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "c2286270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
<<<<<<< HEAD:ch4/chapter4.ipynb
     "execution_count": 18,
=======
     "execution_count": 70,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x =init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe735b",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 19,
=======
   "execution_count": 71,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "b1377e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 20,
=======
   "execution_count": 72,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "6ae5e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 21,
=======
   "execution_count": 73,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "b79e152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:ch4/chapter4.ipynb
      "[[ 0.78133681  0.3657992  -1.95459858]\n",
      " [-0.44369265  0.06322558  1.23157212]]\n",
      "[ 0.0694787   0.27638255 -0.06434424]\n"
=======
      "[[-0.15227052  0.20091773 -0.72043946]\n",
      " [ 0.5920827   1.64711535  1.61901212]]\n",
      "[0.44151212 1.60295445 1.02484723]\n"
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 22,
=======
   "execution_count": 74,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "c85ecea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
<<<<<<< HEAD:ch4/chapter4.ipynb
     "execution_count": 22,
=======
     "execution_count": 74,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 23,
=======
   "execution_count": 75,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "ea94666b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD:ch4/chapter4.ipynb
       "1.2667104424913134"
      ]
     },
     "execution_count": 23,
=======
       "1.206178595597631"
      ]
     },
     "execution_count": 75,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 24,
=======
   "execution_count": 76,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "b48b47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 25,
=======
   "execution_count": 77,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "87a3add0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:ch4/chapter4.ipynb
      "[[ 0.19326097  0.23768477 -0.43094573]\n",
      " [ 0.28989145  0.35652715 -0.6464186 ]]\n"
=======
      "[[ 0.10022476  0.32017176 -0.42039652]\n",
      " [ 0.15033714  0.48025764 -0.63059479]]\n"
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 26,
=======
   "execution_count": 78,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "9e88c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:ch4/chapter4.ipynb
      "[[ 0.19326097  0.23768477 -0.43094573]\n",
      " [ 0.28989145  0.35652715 -0.6464186 ]]\n"
=======
      "[[ 0.10022476  0.32017176 -0.42039652]\n",
      " [ 0.15033714  0.48025764 -0.63059479]]\n"
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2453837",
   "metadata": {},
   "source": [
    "### 학습 알고리즘 구현하기\n",
    "<br>\n",
    "1단계 - 미니배치<br>\n",
    "    훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표다.<br>\n",
    "2단계 - 기울기 산출<br>\n",
    "    미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게하는 방향을 제시한다.<br>\n",
    "3단계 - 배개변수 갱신<br>\n",
    "    가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.<br>\n",
    "4단계 - 반복<br>\n",
    "    1~3단계를 반복한다<br>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 27,
=======
   "execution_count": 79,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "e0f98e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        \n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 28,
=======
   "execution_count": 80,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "c733420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 29,
=======
   "execution_count": 81,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "b1fbfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
<<<<<<< HEAD:ch4/chapter4.ipynb
      "[[0.09173102 0.10093634 0.10937182 0.10273877 0.10222647 0.09898516\n",
      "  0.1003406  0.09593802 0.1002973  0.0974345 ]\n",
      " [0.09178429 0.10079243 0.10960543 0.10259877 0.10253231 0.09893322\n",
      "  0.10032441 0.09618335 0.10061178 0.096634  ]\n",
      " [0.09183511 0.10094106 0.10982765 0.10285821 0.10174276 0.09924425\n",
      "  0.10034772 0.09577355 0.10053357 0.09689612]\n",
      " [0.09190998 0.10093769 0.10943357 0.10265988 0.10216769 0.09867457\n",
      "  0.10053121 0.09621822 0.10070043 0.09676676]\n",
      " [0.09168685 0.10117256 0.1093429  0.10261806 0.10275826 0.09886493\n",
      "  0.10022967 0.0959459  0.10025548 0.0971254 ]\n",
      " [0.09204183 0.1012048  0.10990051 0.10245012 0.10207908 0.09858001\n",
      "  0.10028877 0.09581414 0.1007346  0.09690614]\n",
      " [0.09166442 0.10104282 0.10917174 0.10263493 0.10247291 0.09921692\n",
      "  0.10033897 0.09598252 0.10038906 0.09708571]\n",
      " [0.09181155 0.10109769 0.10983135 0.10256309 0.10175401 0.09912854\n",
      "  0.10041934 0.09595876 0.100585   0.09685067]\n",
      " [0.09172554 0.10112307 0.10941961 0.10268197 0.10206169 0.09888724\n",
      "  0.10041547 0.09611028 0.10034051 0.09723462]\n",
      " [0.09172642 0.10112362 0.10963195 0.10276561 0.10206031 0.09873194\n",
      "  0.10046819 0.09620216 0.1003928  0.096897  ]\n",
      " [0.0916109  0.10077445 0.10985014 0.10257748 0.10202853 0.09908172\n",
      "  0.1003285  0.09622874 0.10075318 0.09676637]\n",
      " [0.09175651 0.10113829 0.10998324 0.10260331 0.10233355 0.09861796\n",
      "  0.09999323 0.09630104 0.1004421  0.09683075]\n",
      " [0.09147137 0.10105498 0.10938222 0.10291425 0.10217366 0.09883252\n",
      "  0.10027685 0.09631361 0.10044038 0.09714017]\n",
      " [0.09152319 0.10146813 0.10953059 0.10261334 0.10242834 0.0988896\n",
      "  0.10049306 0.09568785 0.10031947 0.09704643]\n",
      " [0.09171426 0.10109398 0.10965007 0.10298477 0.10258939 0.09871066\n",
      "  0.10038911 0.09574807 0.10043828 0.09668142]\n",
      " [0.09141827 0.1014422  0.1096289  0.10250724 0.10244965 0.09905378\n",
      "  0.10008118 0.0959616  0.10059998 0.09685721]\n",
      " [0.09182966 0.10098382 0.10980354 0.10279793 0.10242025 0.09884449\n",
      "  0.09999839 0.09612044 0.10017654 0.09702494]\n",
      " [0.09176387 0.10082108 0.10990851 0.10285071 0.10218073 0.09873548\n",
      "  0.10022921 0.0961715  0.10033536 0.09700353]\n",
      " [0.09150538 0.10084109 0.10992996 0.10266021 0.10250389 0.09916717\n",
      "  0.1000675  0.09604232 0.10027941 0.09700307]\n",
      " [0.09177742 0.101017   0.11018839 0.10298409 0.10176664 0.09885299\n",
      "  0.09994818 0.09608558 0.1003133  0.0970664 ]\n",
      " [0.09188212 0.10082025 0.10942713 0.10260223 0.10223388 0.09910859\n",
      "  0.10057272 0.09582773 0.10041092 0.09711442]\n",
      " [0.0917092  0.10096179 0.10957417 0.10280976 0.10192678 0.09910086\n",
      "  0.10037218 0.09623233 0.10045523 0.0968577 ]\n",
      " [0.09149388 0.10091241 0.10984713 0.10292291 0.10236616 0.09873914\n",
      "  0.10006554 0.09604815 0.10075969 0.096845  ]\n",
      " [0.09179864 0.10106823 0.10953325 0.10302757 0.10221115 0.09889557\n",
      "  0.1001344  0.09587818 0.10049281 0.09696019]\n",
      " [0.09180911 0.10083218 0.10951153 0.1029579  0.10267133 0.09890746\n",
      "  0.10005775 0.09602065 0.10018194 0.09705015]\n",
      " [0.09168557 0.10114111 0.10949935 0.10258402 0.10229331 0.09921009\n",
      "  0.10011151 0.09599115 0.10053864 0.09694525]\n",
      " [0.09167022 0.10085698 0.10980657 0.10262607 0.1020073  0.09912558\n",
      "  0.10041072 0.09600587 0.10022951 0.09726118]\n",
      " [0.09166268 0.10110639 0.10950756 0.10263025 0.10242354 0.09880926\n",
      "  0.10049235 0.09608219 0.10071814 0.09656764]\n",
      " [0.09174554 0.10074528 0.10955645 0.10270357 0.1022357  0.0987811\n",
      "  0.10055752 0.09639504 0.10058268 0.09669712]\n",
      " [0.09160975 0.10092524 0.10981496 0.10252923 0.10216427 0.09919872\n",
      "  0.10023377 0.09601549 0.1004197  0.09708887]\n",
      " [0.09187747 0.10115966 0.10956504 0.10232026 0.1025255  0.09911012\n",
      "  0.10019612 0.09554198 0.10065799 0.09704584]\n",
      " [0.09183813 0.10090832 0.109522   0.10260537 0.10222992 0.09903835\n",
      "  0.10039261 0.09609713 0.10064051 0.09672766]\n",
      " [0.091853   0.10094439 0.10961707 0.10269882 0.1021161  0.09912695\n",
      "  0.10052951 0.0959272  0.1002464  0.09694058]\n",
      " [0.09179942 0.10115666 0.10960204 0.10268622 0.10238247 0.09883064\n",
      "  0.10014084 0.09586399 0.1005688  0.09696893]\n",
      " [0.0915797  0.10114991 0.1100357  0.10271982 0.10176976 0.09886223\n",
      "  0.10042747 0.09632611 0.10043726 0.09669204]\n",
      " [0.09147466 0.10083879 0.10969878 0.10287119 0.10203608 0.09880609\n",
      "  0.10048679 0.09612654 0.10090126 0.09675981]\n",
      " [0.09205547 0.10082749 0.10944679 0.10256338 0.10215741 0.09887764\n",
      "  0.1001854  0.09632312 0.10058901 0.09697429]\n",
      " [0.09172092 0.10088359 0.10964579 0.10230182 0.1022839  0.09875774\n",
      "  0.10038966 0.09632399 0.10064198 0.09705061]\n",
      " [0.0917123  0.10099088 0.11009103 0.10226425 0.10203507 0.09889254\n",
      "  0.10060663 0.09593752 0.10067197 0.09679781]\n",
      " [0.09167545 0.10115996 0.10921813 0.10282995 0.10272169 0.09872147\n",
      "  0.10065928 0.09614448 0.10019047 0.09667911]\n",
      " [0.09165866 0.10085795 0.10984966 0.10263503 0.10198081 0.09899201\n",
      "  0.10084144 0.0961615  0.10051455 0.0965084 ]\n",
      " [0.09180228 0.10115404 0.10967832 0.10282095 0.10239347 0.09879569\n",
      "  0.09998161 0.09598004 0.10068932 0.09670427]\n",
      " [0.09175846 0.10095952 0.1095249  0.10233705 0.10182159 0.09887148\n",
      "  0.10078727 0.09603936 0.10097425 0.09692611]\n",
      " [0.09173891 0.10097273 0.10922024 0.10247108 0.10239431 0.09923069\n",
      "  0.10028782 0.09584426 0.10058538 0.09725459]\n",
      " [0.09156098 0.10090511 0.10992356 0.10300547 0.10220349 0.09891082\n",
      "  0.10011323 0.09610924 0.10059969 0.09666839]\n",
      " [0.09161731 0.10079611 0.10969249 0.10272687 0.10230352 0.09863849\n",
      "  0.10046545 0.09621911 0.10056441 0.09697625]\n",
      " [0.09155238 0.10130626 0.10951422 0.10269754 0.1027477  0.09871639\n",
      "  0.10031715 0.095911   0.10053866 0.0966987 ]\n",
      " [0.0918336  0.10105874 0.10945015 0.1026912  0.10229855 0.09879778\n",
      "  0.10051773 0.0958713  0.10041877 0.09706219]\n",
      " [0.09153592 0.10121655 0.10976225 0.10284184 0.10224144 0.09908797\n",
      "  0.09975703 0.09613213 0.10040866 0.09701621]\n",
      " [0.09167097 0.10112502 0.10949557 0.10296569 0.10218014 0.09907279\n",
      "  0.10025079 0.09579924 0.10041794 0.09702185]\n",
      " [0.09182135 0.10125362 0.10965342 0.10299437 0.10226075 0.09879969\n",
      "  0.1001887  0.09611478 0.10036254 0.09655078]\n",
      " [0.09168394 0.10122003 0.10977565 0.1027256  0.10226691 0.09875885\n",
      "  0.09991281 0.09597624 0.10075682 0.09692315]\n",
      " [0.09160089 0.10120454 0.10941408 0.10279025 0.10239155 0.09919413\n",
      "  0.10012818 0.09577536 0.1004588  0.09704223]\n",
      " [0.09181497 0.1008483  0.10923367 0.10256887 0.10215169 0.09914041\n",
      "  0.1006991  0.09603848 0.10073824 0.09676628]\n",
      " [0.09160801 0.10096462 0.10960029 0.10256896 0.1021513  0.09907067\n",
      "  0.10060495 0.09609268 0.1005625  0.09677602]\n",
      " [0.09187604 0.10088602 0.10916376 0.10285488 0.1022877  0.0989887\n",
      "  0.10033168 0.09602275 0.10053642 0.09705205]\n",
      " [0.09198265 0.10095554 0.10940074 0.10259378 0.10222276 0.09923393\n",
      "  0.10001845 0.09605291 0.10051504 0.0970242 ]\n",
      " [0.09161594 0.10097017 0.1096336  0.10267682 0.10219288 0.09894234\n",
      "  0.1005516  0.09588603 0.1006193  0.09691131]\n",
      " [0.09181427 0.1009167  0.10998269 0.1028454  0.1023423  0.09862157\n",
      "  0.10017056 0.09604857 0.10047136 0.09678658]\n",
      " [0.09178485 0.10101733 0.10981084 0.10250786 0.10222    0.09875044\n",
      "  0.10019186 0.0961577  0.10064467 0.09691446]\n",
      " [0.0917479  0.1014832  0.1093571  0.10278946 0.10239378 0.09866591\n",
      "  0.10023679 0.09597545 0.10055615 0.09679427]\n",
      " [0.09177482 0.10118843 0.10950777 0.10245935 0.10227175 0.0993179\n",
      "  0.10032352 0.09586875 0.10047148 0.09681622]\n",
      " [0.09184226 0.100997   0.10975925 0.10238057 0.10210912 0.09907272\n",
      "  0.10031794 0.09608835 0.10053651 0.09689628]\n",
      " [0.09168986 0.10047487 0.10995913 0.10250995 0.10205282 0.09888823\n",
      "  0.1003295  0.09612646 0.10056266 0.09740653]\n",
      " [0.09181356 0.10126575 0.1095022  0.10241375 0.1021845  0.09877786\n",
      "  0.10054488 0.095945   0.10059358 0.09695892]\n",
      " [0.09183767 0.10100328 0.10981981 0.1027455  0.10215387 0.0988507\n",
      "  0.10067319 0.09596526 0.10047979 0.09647094]\n",
      " [0.09195527 0.10124203 0.10932839 0.1024659  0.10187125 0.0991135\n",
      "  0.10036076 0.09627034 0.10044733 0.09694522]\n",
      " [0.09159805 0.10087337 0.11001096 0.10283647 0.10217177 0.09892522\n",
      "  0.09997127 0.09612626 0.10037933 0.0971073 ]\n",
      " [0.09176109 0.10072682 0.11002145 0.10243603 0.10189408 0.09931383\n",
      "  0.10037699 0.09600974 0.10022651 0.09723346]\n",
      " [0.09169197 0.10100395 0.10969377 0.10253111 0.10209253 0.09882004\n",
      "  0.10014491 0.09614605 0.10076323 0.09711243]\n",
      " [0.09184994 0.10128485 0.10911085 0.10292538 0.10244219 0.0988369\n",
      "  0.10014732 0.09578223 0.1004073  0.09721304]\n",
      " [0.09203709 0.10096227 0.10993327 0.10269092 0.10192043 0.09888309\n",
      "  0.10003925 0.09612521 0.10032838 0.0970801 ]\n",
      " [0.09151366 0.10118484 0.11011769 0.10238573 0.10214675 0.09923644\n",
      "  0.1003666  0.09604397 0.10038058 0.09662373]\n",
      " [0.09189268 0.10099764 0.10947071 0.10272007 0.10201909 0.09885558\n",
      "  0.10038232 0.0963084  0.10044224 0.09691126]\n",
      " [0.09170815 0.10104222 0.1098654  0.10248174 0.10179371 0.09901511\n",
      "  0.10029165 0.09633954 0.10041671 0.09704576]\n",
      " [0.09190961 0.10101847 0.10948062 0.10293902 0.10240792 0.09860082\n",
      "  0.10006495 0.09619388 0.10021303 0.09717168]\n",
      " [0.09171806 0.10066386 0.10961096 0.10282703 0.10238424 0.09884055\n",
      "  0.10052893 0.09576037 0.10047761 0.09718838]\n",
      " [0.09177032 0.10085614 0.10970047 0.10232959 0.10199554 0.09909482\n",
      "  0.10022381 0.09613654 0.1008122  0.09708057]\n",
      " [0.09138635 0.10111719 0.10984175 0.10256361 0.10224679 0.09918894\n",
      "  0.10017811 0.09591983 0.10059639 0.09696102]\n",
      " [0.09192254 0.1010241  0.10974895 0.10226684 0.10246628 0.09898124\n",
      "  0.10034551 0.09587673 0.10041741 0.0969504 ]\n",
      " [0.09174362 0.10088317 0.10995081 0.10271531 0.10231926 0.09892491\n",
      "  0.10012378 0.09605665 0.10048487 0.09679761]\n",
      " [0.09187964 0.10110011 0.10902903 0.10272073 0.10232812 0.09859052\n",
      "  0.10058763 0.09630271 0.10061047 0.09685104]\n",
      " [0.09189909 0.1008496  0.10972286 0.10275165 0.10185462 0.0990306\n",
      "  0.10023356 0.09623332 0.10043668 0.09698801]\n",
      " [0.09199205 0.1009994  0.10936041 0.10274919 0.10217034 0.09899068\n",
      "  0.10040634 0.09588631 0.10061193 0.09683335]\n",
      " [0.09175171 0.10105202 0.10959318 0.1029399  0.10217891 0.09870017\n",
      "  0.10011512 0.09603213 0.10062021 0.09701666]\n",
      " [0.09171634 0.10137861 0.10977544 0.10278181 0.10212107 0.09914797\n",
      "  0.1001642  0.09612251 0.10029732 0.09649474]\n",
      " [0.09183976 0.10117179 0.10909668 0.10260769 0.10206981 0.0988061\n",
      "  0.10055033 0.0961265  0.10058861 0.09714273]\n",
      " [0.09182747 0.1008729  0.10983377 0.10254641 0.10216179 0.09885719\n",
      "  0.10032273 0.09628442 0.1004743  0.096819  ]\n",
      " [0.09174791 0.10083647 0.10922138 0.10287667 0.10223421 0.09884383\n",
      "  0.10030614 0.09635107 0.10062655 0.09695575]\n",
      " [0.09188161 0.1010531  0.10983357 0.10237556 0.10211672 0.09900443\n",
      "  0.10041851 0.0960879  0.10041041 0.09681819]\n",
      " [0.09166937 0.10079142 0.109566   0.1030565  0.10218052 0.09880399\n",
      "  0.10019224 0.09613394 0.10064938 0.09695663]\n",
      " [0.09182335 0.10091297 0.1095131  0.10240878 0.10220802 0.09891003\n",
      "  0.10060318 0.09613926 0.10040487 0.09707646]\n",
      " [0.09169706 0.1013711  0.10973998 0.10254954 0.10204123 0.09890558\n",
      "  0.10051078 0.09587343 0.1003748  0.0969365 ]\n",
      " [0.09169544 0.10082988 0.10923049 0.10272428 0.10242324 0.09893638\n",
      "  0.1003805  0.09595692 0.10057843 0.09724444]\n",
      " [0.09165463 0.10161827 0.10890168 0.10275111 0.10257526 0.09896731\n",
      "  0.10043907 0.0956145  0.10059375 0.09688443]\n",
      " [0.09162215 0.10102503 0.10958674 0.1027239  0.10188579 0.09887193\n",
      "  0.10048418 0.09593947 0.10095354 0.09690728]\n",
      " [0.0919576  0.10087487 0.10911927 0.10276954 0.1023163  0.0992259\n",
      "  0.10038637 0.0958438  0.10044211 0.09706425]\n",
      " [0.09180277 0.10086814 0.1096137  0.10259481 0.10236263 0.09877978\n",
      "  0.10014416 0.09605109 0.10058052 0.09720241]\n",
      " [0.09155412 0.10106153 0.10940196 0.10259258 0.10232137 0.09905494\n",
      "  0.10029846 0.09643371 0.10054346 0.09673787]\n",
      " [0.09182923 0.10137382 0.10926818 0.10301838 0.10228421 0.09882774\n",
      "  0.10021182 0.09586272 0.10052359 0.09680031]]\n"
=======
      "[[0.10974682 0.09439177 0.10177043 0.10390326 0.09840412 0.09645367\n",
      "  0.10271975 0.09956832 0.094254   0.09878786]\n",
      " [0.10964726 0.09439468 0.10187711 0.10410224 0.09809079 0.09624538\n",
      "  0.10309977 0.09946119 0.09421052 0.09887105]\n",
      " [0.10967416 0.09426623 0.10173003 0.10388469 0.09819968 0.09645691\n",
      "  0.10285107 0.09956612 0.09449645 0.09887465]\n",
      " [0.10970701 0.09389491 0.10205573 0.10380107 0.0983899  0.09600368\n",
      "  0.10303575 0.09935911 0.09456159 0.09919126]\n",
      " [0.10957784 0.09407114 0.102051   0.1037048  0.09855015 0.09630384\n",
      "  0.10283091 0.09991095 0.0941159  0.09888348]\n",
      " [0.10950338 0.09437998 0.10200847 0.10367001 0.09850385 0.09661346\n",
      "  0.10283692 0.09946442 0.09411758 0.09890193]\n",
      " [0.10994597 0.09424309 0.10191198 0.10406129 0.0981863  0.09631613\n",
      "  0.10243091 0.09985732 0.09410177 0.09894525]\n",
      " [0.10978541 0.09373898 0.10224302 0.103929   0.09821973 0.09616488\n",
      "  0.10299845 0.09984115 0.09444122 0.09863816]\n",
      " [0.10985199 0.09402743 0.10181759 0.10418553 0.09824191 0.09615694\n",
      "  0.10303113 0.09944496 0.09447015 0.09877236]\n",
      " [0.10966579 0.09415113 0.10234782 0.10410747 0.09819702 0.09623833\n",
      "  0.10321888 0.0997273  0.09430696 0.09803931]\n",
      " [0.10988624 0.09397004 0.10237804 0.10417039 0.09828057 0.09585933\n",
      "  0.10267359 0.09991163 0.09433361 0.09853655]\n",
      " [0.10960004 0.09404124 0.10206613 0.10421061 0.09839958 0.0967727\n",
      "  0.10243981 0.09960124 0.09405275 0.0988159 ]\n",
      " [0.11011704 0.09402086 0.10196206 0.10407856 0.09815983 0.09650092\n",
      "  0.10277165 0.09946686 0.09416411 0.0987581 ]\n",
      " [0.10966147 0.09405284 0.10222875 0.10392515 0.09845947 0.09631082\n",
      "  0.10282952 0.09933064 0.09410531 0.09909603]\n",
      " [0.10950348 0.09446877 0.1019874  0.10376719 0.09795234 0.09641193\n",
      "  0.10292986 0.09958496 0.0943033  0.09909077]\n",
      " [0.10970057 0.09402205 0.10167818 0.1040985  0.09837059 0.09603439\n",
      "  0.103071   0.09979176 0.09423459 0.09899836]\n",
      " [0.10990955 0.0940676  0.10183393 0.10395615 0.09833236 0.09617928\n",
      "  0.1028095  0.09984662 0.09445576 0.09860927]\n",
      " [0.10982832 0.09410134 0.1021679  0.10345001 0.09804541 0.09647505\n",
      "  0.10284617 0.09954358 0.09439313 0.0991491 ]\n",
      " [0.10969416 0.09399623 0.10173004 0.10381933 0.09872995 0.09635915\n",
      "  0.10306993 0.0992489  0.09419763 0.09915468]\n",
      " [0.10952988 0.09404051 0.1021712  0.10407182 0.0981957  0.09627445\n",
      "  0.10272178 0.09980513 0.09466135 0.09852818]\n",
      " [0.10978736 0.09415342 0.10168417 0.10387506 0.09867711 0.09635057\n",
      "  0.10284257 0.09960935 0.09428232 0.09873806]\n",
      " [0.1097546  0.09435492 0.1019724  0.10431943 0.09787348 0.09631707\n",
      "  0.10265887 0.09965226 0.09441444 0.09868253]\n",
      " [0.11002378 0.09421164 0.10202258 0.10412656 0.09801283 0.09619522\n",
      "  0.1027118  0.09978657 0.09427739 0.09863162]\n",
      " [0.10951436 0.09390882 0.10250788 0.10384673 0.09816689 0.0963466\n",
      "  0.10285894 0.09957528 0.09453416 0.09874035]\n",
      " [0.10954436 0.0939979  0.10196    0.10403355 0.09813145 0.0964399\n",
      "  0.10289397 0.09969777 0.09413729 0.09916381]\n",
      " [0.1095252  0.09380934 0.10218802 0.10443954 0.09819579 0.09638589\n",
      "  0.10260234 0.09983918 0.09425888 0.09875584]\n",
      " [0.10968157 0.09462453 0.10168715 0.10398585 0.0980806  0.09655682\n",
      "  0.10282102 0.09972423 0.09414211 0.09869612]\n",
      " [0.10958741 0.09400698 0.10199473 0.10398396 0.09817879 0.09599816\n",
      "  0.10324129 0.09981795 0.09439733 0.09879339]\n",
      " [0.10979093 0.0939069  0.10225227 0.10416777 0.09826343 0.09630337\n",
      "  0.10285065 0.09936398 0.09452212 0.09857859]\n",
      " [0.10965862 0.0938449  0.10171174 0.10390739 0.09877867 0.09618365\n",
      "  0.10299905 0.09942848 0.09449538 0.09899213]\n",
      " [0.10980966 0.09432567 0.1018747  0.10384273 0.09800041 0.09657754\n",
      "  0.1029517  0.09968902 0.09406215 0.09886642]\n",
      " [0.10985666 0.09421495 0.10206428 0.10413199 0.09781808 0.09612517\n",
      "  0.10290897 0.09973888 0.09414589 0.09899513]\n",
      " [0.10973304 0.09427176 0.10201381 0.10416184 0.09808816 0.09615704\n",
      "  0.10282407 0.09973389 0.09444251 0.09857388]\n",
      " [0.10981456 0.09390345 0.10233356 0.10419978 0.09811819 0.09621919\n",
      "  0.10284466 0.09942726 0.0941832  0.09895614]\n",
      " [0.10963666 0.09412522 0.10177482 0.10376657 0.0986799  0.09644599\n",
      "  0.10279255 0.09956249 0.0942523  0.09896351]\n",
      " [0.10961289 0.0941374  0.10227517 0.10409457 0.09807706 0.09612315\n",
      "  0.10304953 0.09960912 0.09428869 0.09873241]\n",
      " [0.10956003 0.09409953 0.10190182 0.10389552 0.09841548 0.09659926\n",
      "  0.10305457 0.09930978 0.09441375 0.09875028]\n",
      " [0.10990144 0.09403647 0.10193462 0.10446552 0.09816869 0.09610707\n",
      "  0.10262509 0.09948408 0.09459786 0.09867916]\n",
      " [0.11005631 0.09405714 0.10237648 0.10423449 0.09808595 0.09606479\n",
      "  0.10248231 0.09960413 0.09433687 0.09870152]\n",
      " [0.10985107 0.09445453 0.10174907 0.10410895 0.09832583 0.09644084\n",
      "  0.10289627 0.0991764  0.09426099 0.09873605]\n",
      " [0.10979289 0.09416343 0.10216471 0.10405519 0.09820727 0.09646121\n",
      "  0.10309583 0.09957006 0.09404815 0.09844126]\n",
      " [0.10943955 0.09415193 0.10180106 0.10403039 0.09840254 0.09636482\n",
      "  0.10279684 0.09966606 0.09450075 0.09884605]\n",
      " [0.10983088 0.0938259  0.1021833  0.10432635 0.09827097 0.09635637\n",
      "  0.10277381 0.0993802  0.09431679 0.09873543]\n",
      " [0.11013845 0.09402099 0.10210279 0.10361496 0.09847122 0.09671497\n",
      "  0.1029486  0.09942889 0.09404568 0.09851346]\n",
      " [0.10946053 0.09406213 0.10191134 0.10398021 0.09836347 0.09636274\n",
      "  0.10283158 0.09996391 0.09421102 0.09885307]\n",
      " [0.10987761 0.09424494 0.1022827  0.10342629 0.098194   0.09643959\n",
      "  0.10338871 0.09945474 0.0940105  0.09868092]\n",
      " [0.1095885  0.09390512 0.10208918 0.10407741 0.09836952 0.09625923\n",
      "  0.10284315 0.09973736 0.09434371 0.09878681]\n",
      " [0.10975502 0.09467052 0.10163435 0.10381441 0.09834848 0.09646603\n",
      "  0.10265865 0.09954469 0.09424637 0.09886149]\n",
      " [0.10941083 0.09437573 0.10230506 0.10385885 0.098009   0.09636022\n",
      "  0.10292451 0.09916639 0.09456688 0.09902253]\n",
      " [0.10944596 0.09455362 0.10168618 0.10431579 0.09817967 0.09642398\n",
      "  0.10280172 0.09970843 0.09414362 0.09874103]\n",
      " [0.11028766 0.09406283 0.10175953 0.10411046 0.09856478 0.09604009\n",
      "  0.10245074 0.09977879 0.09421112 0.098734  ]\n",
      " [0.10937121 0.09417225 0.10213592 0.10384238 0.09838874 0.09631301\n",
      "  0.10287273 0.099704   0.09437179 0.09882796]\n",
      " [0.11005983 0.09435328 0.10232028 0.10417161 0.09817611 0.09585704\n",
      "  0.10281421 0.09962357 0.09404224 0.09858183]\n",
      " [0.10945284 0.09424459 0.10235471 0.10420261 0.09814026 0.09641452\n",
      "  0.10288845 0.09923728 0.09455031 0.09851443]\n",
      " [0.10968056 0.09374832 0.10232947 0.10399016 0.09822656 0.09644971\n",
      "  0.10293565 0.09943973 0.09440912 0.09879072]\n",
      " [0.10943409 0.09449297 0.10215444 0.10381212 0.09810558 0.09615445\n",
      "  0.10286338 0.09992742 0.0942785  0.09877706]\n",
      " [0.10977439 0.09419279 0.10201626 0.10409893 0.09847234 0.09610482\n",
      "  0.10307853 0.09933805 0.09434995 0.09857395]\n",
      " [0.10981017 0.09402515 0.10205106 0.10438243 0.09833192 0.09600068\n",
      "  0.10283985 0.09962503 0.09413986 0.09879386]\n",
      " [0.10944829 0.09406101 0.10202915 0.10377249 0.09840265 0.09598733\n",
      "  0.10322547 0.0998507  0.09456691 0.098656  ]\n",
      " [0.10971334 0.09427007 0.10218112 0.10397763 0.09831645 0.09653773\n",
      "  0.10275346 0.09949965 0.09415121 0.09859934]\n",
      " [0.1095606  0.09435987 0.10163963 0.1040132  0.09828095 0.09633227\n",
      "  0.10305996 0.09989346 0.09414421 0.09871586]\n",
      " [0.10960092 0.09461591 0.10208577 0.10356047 0.09805045 0.09632327\n",
      "  0.10302783 0.09965106 0.09433219 0.09875214]\n",
      " [0.10934069 0.09423132 0.10218883 0.10403454 0.09818091 0.09628095\n",
      "  0.10295242 0.09969737 0.09419147 0.09890151]\n",
      " [0.10946624 0.0940925  0.10186128 0.10433339 0.0983776  0.0961371\n",
      "  0.10263696 0.09989223 0.09437551 0.09882719]\n",
      " [0.10990637 0.09417145 0.10176295 0.10415436 0.09841271 0.09612957\n",
      "  0.10236068 0.0998134  0.09444361 0.09884491]\n",
      " [0.10970446 0.09432812 0.10202531 0.10392572 0.09783022 0.09620919\n",
      "  0.10303711 0.09978566 0.09437485 0.09877936]\n",
      " [0.109765   0.09423041 0.10216188 0.10386427 0.09834639 0.09617224\n",
      "  0.10276244 0.09950308 0.09441027 0.09878402]\n",
      " [0.10946984 0.09434706 0.10219586 0.10409429 0.09819584 0.09613625\n",
      "  0.10288426 0.0995342  0.09446836 0.09867404]\n",
      " [0.10967129 0.09403966 0.10232319 0.10378339 0.09845901 0.09623052\n",
      "  0.1029863  0.09955653 0.09406921 0.09888089]\n",
      " [0.10958809 0.09393801 0.10244568 0.10410094 0.09824153 0.09630243\n",
      "  0.10319833 0.09944809 0.09435809 0.09837881]\n",
      " [0.10984851 0.0939359  0.10211328 0.10440216 0.09797351 0.09599834\n",
      "  0.10292401 0.09966408 0.0943913  0.09874891]\n",
      " [0.10955354 0.0940799  0.10209463 0.10394826 0.0983532  0.09651683\n",
      "  0.10304317 0.09952793 0.09438429 0.09849825]\n",
      " [0.10979716 0.09395606 0.10210111 0.1041947  0.0982842  0.09640881\n",
      "  0.10264296 0.09958814 0.09426256 0.09876432]\n",
      " [0.10971788 0.09405379 0.10185912 0.10392425 0.09866262 0.09632623\n",
      "  0.10261047 0.09949003 0.0945311  0.09882451]\n",
      " [0.10967434 0.0941804  0.10220511 0.10417977 0.09827577 0.09599495\n",
      "  0.10300062 0.0995204  0.09425896 0.09870969]\n",
      " [0.11022234 0.09434859 0.1022843  0.10392573 0.09756367 0.09616763\n",
      "  0.10277071 0.09981776 0.09425107 0.0986482 ]\n",
      " [0.10996678 0.09439286 0.1020122  0.10416258 0.0978859  0.0963984\n",
      "  0.102775   0.09944221 0.09426541 0.09869865]\n",
      " [0.10981604 0.09432914 0.10170929 0.10421644 0.09827908 0.09616119\n",
      "  0.10273613 0.0995431  0.09453811 0.09867146]\n",
      " [0.1097463  0.09436408 0.10212606 0.10410037 0.09832795 0.09633533\n",
      "  0.10251333 0.09948189 0.09449173 0.09851295]\n",
      " [0.1098619  0.09430802 0.10191155 0.10414159 0.09801578 0.09633267\n",
      "  0.10281077 0.09934685 0.09427359 0.09899727]\n",
      " [0.10953867 0.09429904 0.10212635 0.10397634 0.09837723 0.09595265\n",
      "  0.10307501 0.09952431 0.09446892 0.09866147]\n",
      " [0.10960382 0.09405602 0.10228976 0.10402079 0.09837033 0.0964377\n",
      "  0.10308922 0.09924493 0.09431976 0.09856767]\n",
      " [0.10938919 0.09417701 0.10205862 0.10436847 0.09802009 0.09634709\n",
      "  0.10260261 0.09975201 0.09439918 0.09888572]\n",
      " [0.10996287 0.09402731 0.10224567 0.1038699  0.09837673 0.09654996\n",
      "  0.10274106 0.09955562 0.09428953 0.09838135]\n",
      " [0.1093399  0.09420078 0.10181598 0.10404818 0.09833701 0.09625458\n",
      "  0.10283858 0.09967039 0.09451458 0.09898003]\n",
      " [0.10976978 0.0938521  0.10211041 0.1043528  0.09819053 0.09625778\n",
      "  0.10263032 0.09976799 0.09420828 0.09886   ]\n",
      " [0.10981293 0.09436863 0.10152888 0.10412064 0.09815469 0.09651558\n",
      "  0.10260134 0.09964016 0.0944104  0.09884676]\n",
      " [0.10952898 0.09403282 0.10192914 0.10418617 0.09833002 0.0960009\n",
      "  0.10306954 0.09981328 0.09428089 0.09882826]\n",
      " [0.10995608 0.09412793 0.10182073 0.10420386 0.09853724 0.09637118\n",
      "  0.10268346 0.09951711 0.09410115 0.09868124]\n",
      " [0.10944094 0.09442029 0.10212786 0.10378684 0.09809015 0.09629191\n",
      "  0.10274504 0.09953566 0.09457526 0.09898606]\n",
      " [0.10984765 0.0940571  0.10203782 0.10416716 0.09799928 0.09641233\n",
      "  0.10279767 0.0994886  0.09443251 0.0987599 ]\n",
      " [0.1093826  0.09441136 0.10203326 0.10408671 0.09821671 0.09624755\n",
      "  0.1030433  0.09973468 0.09422441 0.09861944]\n",
      " [0.10959333 0.09412126 0.10213485 0.10416259 0.09810718 0.09633759\n",
      "  0.10274777 0.09963524 0.09443497 0.09872522]\n",
      " [0.10952807 0.09431644 0.10222765 0.10442423 0.09813359 0.09636875\n",
      "  0.10293631 0.0994256  0.09410419 0.09853517]\n",
      " [0.10949905 0.09425692 0.10223332 0.10376273 0.09837763 0.09632124\n",
      "  0.10288868 0.09973866 0.09406039 0.09886139]\n",
      " [0.10979042 0.09410435 0.10242184 0.10404315 0.09843424 0.09607259\n",
      "  0.10286406 0.09973018 0.09396863 0.09857053]\n",
      " [0.10952879 0.09428973 0.10185741 0.10396172 0.09813219 0.09644765\n",
      "  0.10267188 0.09969448 0.09436888 0.09904726]\n",
      " [0.10957764 0.09402638 0.10213731 0.10401299 0.09839185 0.09636477\n",
      "  0.10287847 0.09937576 0.0942649  0.09896994]\n",
      " [0.10961709 0.09421593 0.10210201 0.10394056 0.09841245 0.09638765\n",
      "  0.10291955 0.09957726 0.09420482 0.09862269]\n",
      " [0.10950599 0.09421615 0.10216705 0.10437848 0.09833523 0.0962007\n",
      "  0.10281281 0.09956488 0.09442202 0.09839669]]\n"
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "print(x.shape)\n",
    "y = net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 30,
=======
   "execution_count": 82,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "9104f1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/1259853793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep_learning/ch4/../common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfxh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x+h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep_learning/ch4/../common/functions.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:ch4/chapter4.ipynb
   "execution_count": 31,
=======
   "execution_count": 84,
>>>>>>> decfda4f5aa1da1cce83b0e26e89609cc1e5f5f0:ch4/Untitled.ipynb
   "id": "7bc746f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/2059443021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep_learning/ch4/../common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfxh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x+h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/39/2knbgcdd14xfvt_5n7_wl0dh0000gn/T/ipykernel_23918/4147064376.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 미니배치 학습 구현\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train) , (x_test, t_test) = load_mnist(normalize=True, one_hot_label= True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 25\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "lr = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= lr * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 학습 구현\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ead02c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[0;32m     16\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[1;32m---> 18\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     21\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad[key]\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\two_layer_net.py:48\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     45\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     47\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 48\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     50\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\..\\common\\gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m tmp_val \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[0;32m     42\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tmp_val) \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m---> 43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[0;32m     46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\two_layer_net.py:45\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 45\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     48\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\two_layer_net.py:33\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     31\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcross_entropy_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\..\\common\\functions.py:56\u001b[0m, in \u001b[0;36mcross_entropy_error\u001b[1;34m(y, t)\u001b[0m\n\u001b[0;32m     53\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)) \u001b[38;5;241m/\u001b[39m batch_size\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "lr = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    \n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = x_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= lr * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
