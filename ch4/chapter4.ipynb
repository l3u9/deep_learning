{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557585c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebd1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 제곱 오차\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afd4dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4397e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "# log(0)은 inf기 때문에 임의의 작은 값 delta를 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b60cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04174e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462a5161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  60000\n",
      "[51852  7468 25842  9581 47999 56952 57235 50168  7848   710]\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "print(\"train_size: \", train_size)\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(len(x_batch))\n",
    "print(len(t_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2b6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치용 교차 엔트로피 구현\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+ 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6297e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 핫 인코딩이 아닌 숫자 레이블로 주어졌을 때 엔트로피 오차\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64c874",
   "metadata": {},
   "source": [
    "### 수지해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0e46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83a2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d4b96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa104e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e456b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce97269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb421ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49be7243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 큰 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x =init_x, lr=10.0, step_num =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2286270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x =init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe735b",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1377e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ae5e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b79e152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.78133681  0.3657992  -1.95459858]\n",
      " [-0.44369265  0.06322558  1.23157212]]\n",
      "[ 0.0694787   0.27638255 -0.06434424]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c85ecea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea94666b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2667104424913134"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b48b47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87a3add0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19326097  0.23768477 -0.43094573]\n",
      " [ 0.28989145  0.35652715 -0.6464186 ]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e88c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19326097  0.23768477 -0.43094573]\n",
      " [ 0.28989145  0.35652715 -0.6464186 ]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2453837",
   "metadata": {},
   "source": [
    "### 학습 알고리즘 구현하기\n",
    "<br>\n",
    "1단계 - 미니배치<br>\n",
    "    훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표다.<br>\n",
    "2단계 - 기울기 산출<br>\n",
    "    미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게하는 방향을 제시한다.<br>\n",
    "3단계 - 배개변수 갱신<br>\n",
    "    가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.<br>\n",
    "4단계 - 반복<br>\n",
    "    1~3단계를 반복한다<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0f98e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        \n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c733420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1fbfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "[[0.09173102 0.10093634 0.10937182 0.10273877 0.10222647 0.09898516\n",
      "  0.1003406  0.09593802 0.1002973  0.0974345 ]\n",
      " [0.09178429 0.10079243 0.10960543 0.10259877 0.10253231 0.09893322\n",
      "  0.10032441 0.09618335 0.10061178 0.096634  ]\n",
      " [0.09183511 0.10094106 0.10982765 0.10285821 0.10174276 0.09924425\n",
      "  0.10034772 0.09577355 0.10053357 0.09689612]\n",
      " [0.09190998 0.10093769 0.10943357 0.10265988 0.10216769 0.09867457\n",
      "  0.10053121 0.09621822 0.10070043 0.09676676]\n",
      " [0.09168685 0.10117256 0.1093429  0.10261806 0.10275826 0.09886493\n",
      "  0.10022967 0.0959459  0.10025548 0.0971254 ]\n",
      " [0.09204183 0.1012048  0.10990051 0.10245012 0.10207908 0.09858001\n",
      "  0.10028877 0.09581414 0.1007346  0.09690614]\n",
      " [0.09166442 0.10104282 0.10917174 0.10263493 0.10247291 0.09921692\n",
      "  0.10033897 0.09598252 0.10038906 0.09708571]\n",
      " [0.09181155 0.10109769 0.10983135 0.10256309 0.10175401 0.09912854\n",
      "  0.10041934 0.09595876 0.100585   0.09685067]\n",
      " [0.09172554 0.10112307 0.10941961 0.10268197 0.10206169 0.09888724\n",
      "  0.10041547 0.09611028 0.10034051 0.09723462]\n",
      " [0.09172642 0.10112362 0.10963195 0.10276561 0.10206031 0.09873194\n",
      "  0.10046819 0.09620216 0.1003928  0.096897  ]\n",
      " [0.0916109  0.10077445 0.10985014 0.10257748 0.10202853 0.09908172\n",
      "  0.1003285  0.09622874 0.10075318 0.09676637]\n",
      " [0.09175651 0.10113829 0.10998324 0.10260331 0.10233355 0.09861796\n",
      "  0.09999323 0.09630104 0.1004421  0.09683075]\n",
      " [0.09147137 0.10105498 0.10938222 0.10291425 0.10217366 0.09883252\n",
      "  0.10027685 0.09631361 0.10044038 0.09714017]\n",
      " [0.09152319 0.10146813 0.10953059 0.10261334 0.10242834 0.0988896\n",
      "  0.10049306 0.09568785 0.10031947 0.09704643]\n",
      " [0.09171426 0.10109398 0.10965007 0.10298477 0.10258939 0.09871066\n",
      "  0.10038911 0.09574807 0.10043828 0.09668142]\n",
      " [0.09141827 0.1014422  0.1096289  0.10250724 0.10244965 0.09905378\n",
      "  0.10008118 0.0959616  0.10059998 0.09685721]\n",
      " [0.09182966 0.10098382 0.10980354 0.10279793 0.10242025 0.09884449\n",
      "  0.09999839 0.09612044 0.10017654 0.09702494]\n",
      " [0.09176387 0.10082108 0.10990851 0.10285071 0.10218073 0.09873548\n",
      "  0.10022921 0.0961715  0.10033536 0.09700353]\n",
      " [0.09150538 0.10084109 0.10992996 0.10266021 0.10250389 0.09916717\n",
      "  0.1000675  0.09604232 0.10027941 0.09700307]\n",
      " [0.09177742 0.101017   0.11018839 0.10298409 0.10176664 0.09885299\n",
      "  0.09994818 0.09608558 0.1003133  0.0970664 ]\n",
      " [0.09188212 0.10082025 0.10942713 0.10260223 0.10223388 0.09910859\n",
      "  0.10057272 0.09582773 0.10041092 0.09711442]\n",
      " [0.0917092  0.10096179 0.10957417 0.10280976 0.10192678 0.09910086\n",
      "  0.10037218 0.09623233 0.10045523 0.0968577 ]\n",
      " [0.09149388 0.10091241 0.10984713 0.10292291 0.10236616 0.09873914\n",
      "  0.10006554 0.09604815 0.10075969 0.096845  ]\n",
      " [0.09179864 0.10106823 0.10953325 0.10302757 0.10221115 0.09889557\n",
      "  0.1001344  0.09587818 0.10049281 0.09696019]\n",
      " [0.09180911 0.10083218 0.10951153 0.1029579  0.10267133 0.09890746\n",
      "  0.10005775 0.09602065 0.10018194 0.09705015]\n",
      " [0.09168557 0.10114111 0.10949935 0.10258402 0.10229331 0.09921009\n",
      "  0.10011151 0.09599115 0.10053864 0.09694525]\n",
      " [0.09167022 0.10085698 0.10980657 0.10262607 0.1020073  0.09912558\n",
      "  0.10041072 0.09600587 0.10022951 0.09726118]\n",
      " [0.09166268 0.10110639 0.10950756 0.10263025 0.10242354 0.09880926\n",
      "  0.10049235 0.09608219 0.10071814 0.09656764]\n",
      " [0.09174554 0.10074528 0.10955645 0.10270357 0.1022357  0.0987811\n",
      "  0.10055752 0.09639504 0.10058268 0.09669712]\n",
      " [0.09160975 0.10092524 0.10981496 0.10252923 0.10216427 0.09919872\n",
      "  0.10023377 0.09601549 0.1004197  0.09708887]\n",
      " [0.09187747 0.10115966 0.10956504 0.10232026 0.1025255  0.09911012\n",
      "  0.10019612 0.09554198 0.10065799 0.09704584]\n",
      " [0.09183813 0.10090832 0.109522   0.10260537 0.10222992 0.09903835\n",
      "  0.10039261 0.09609713 0.10064051 0.09672766]\n",
      " [0.091853   0.10094439 0.10961707 0.10269882 0.1021161  0.09912695\n",
      "  0.10052951 0.0959272  0.1002464  0.09694058]\n",
      " [0.09179942 0.10115666 0.10960204 0.10268622 0.10238247 0.09883064\n",
      "  0.10014084 0.09586399 0.1005688  0.09696893]\n",
      " [0.0915797  0.10114991 0.1100357  0.10271982 0.10176976 0.09886223\n",
      "  0.10042747 0.09632611 0.10043726 0.09669204]\n",
      " [0.09147466 0.10083879 0.10969878 0.10287119 0.10203608 0.09880609\n",
      "  0.10048679 0.09612654 0.10090126 0.09675981]\n",
      " [0.09205547 0.10082749 0.10944679 0.10256338 0.10215741 0.09887764\n",
      "  0.1001854  0.09632312 0.10058901 0.09697429]\n",
      " [0.09172092 0.10088359 0.10964579 0.10230182 0.1022839  0.09875774\n",
      "  0.10038966 0.09632399 0.10064198 0.09705061]\n",
      " [0.0917123  0.10099088 0.11009103 0.10226425 0.10203507 0.09889254\n",
      "  0.10060663 0.09593752 0.10067197 0.09679781]\n",
      " [0.09167545 0.10115996 0.10921813 0.10282995 0.10272169 0.09872147\n",
      "  0.10065928 0.09614448 0.10019047 0.09667911]\n",
      " [0.09165866 0.10085795 0.10984966 0.10263503 0.10198081 0.09899201\n",
      "  0.10084144 0.0961615  0.10051455 0.0965084 ]\n",
      " [0.09180228 0.10115404 0.10967832 0.10282095 0.10239347 0.09879569\n",
      "  0.09998161 0.09598004 0.10068932 0.09670427]\n",
      " [0.09175846 0.10095952 0.1095249  0.10233705 0.10182159 0.09887148\n",
      "  0.10078727 0.09603936 0.10097425 0.09692611]\n",
      " [0.09173891 0.10097273 0.10922024 0.10247108 0.10239431 0.09923069\n",
      "  0.10028782 0.09584426 0.10058538 0.09725459]\n",
      " [0.09156098 0.10090511 0.10992356 0.10300547 0.10220349 0.09891082\n",
      "  0.10011323 0.09610924 0.10059969 0.09666839]\n",
      " [0.09161731 0.10079611 0.10969249 0.10272687 0.10230352 0.09863849\n",
      "  0.10046545 0.09621911 0.10056441 0.09697625]\n",
      " [0.09155238 0.10130626 0.10951422 0.10269754 0.1027477  0.09871639\n",
      "  0.10031715 0.095911   0.10053866 0.0966987 ]\n",
      " [0.0918336  0.10105874 0.10945015 0.1026912  0.10229855 0.09879778\n",
      "  0.10051773 0.0958713  0.10041877 0.09706219]\n",
      " [0.09153592 0.10121655 0.10976225 0.10284184 0.10224144 0.09908797\n",
      "  0.09975703 0.09613213 0.10040866 0.09701621]\n",
      " [0.09167097 0.10112502 0.10949557 0.10296569 0.10218014 0.09907279\n",
      "  0.10025079 0.09579924 0.10041794 0.09702185]\n",
      " [0.09182135 0.10125362 0.10965342 0.10299437 0.10226075 0.09879969\n",
      "  0.1001887  0.09611478 0.10036254 0.09655078]\n",
      " [0.09168394 0.10122003 0.10977565 0.1027256  0.10226691 0.09875885\n",
      "  0.09991281 0.09597624 0.10075682 0.09692315]\n",
      " [0.09160089 0.10120454 0.10941408 0.10279025 0.10239155 0.09919413\n",
      "  0.10012818 0.09577536 0.1004588  0.09704223]\n",
      " [0.09181497 0.1008483  0.10923367 0.10256887 0.10215169 0.09914041\n",
      "  0.1006991  0.09603848 0.10073824 0.09676628]\n",
      " [0.09160801 0.10096462 0.10960029 0.10256896 0.1021513  0.09907067\n",
      "  0.10060495 0.09609268 0.1005625  0.09677602]\n",
      " [0.09187604 0.10088602 0.10916376 0.10285488 0.1022877  0.0989887\n",
      "  0.10033168 0.09602275 0.10053642 0.09705205]\n",
      " [0.09198265 0.10095554 0.10940074 0.10259378 0.10222276 0.09923393\n",
      "  0.10001845 0.09605291 0.10051504 0.0970242 ]\n",
      " [0.09161594 0.10097017 0.1096336  0.10267682 0.10219288 0.09894234\n",
      "  0.1005516  0.09588603 0.1006193  0.09691131]\n",
      " [0.09181427 0.1009167  0.10998269 0.1028454  0.1023423  0.09862157\n",
      "  0.10017056 0.09604857 0.10047136 0.09678658]\n",
      " [0.09178485 0.10101733 0.10981084 0.10250786 0.10222    0.09875044\n",
      "  0.10019186 0.0961577  0.10064467 0.09691446]\n",
      " [0.0917479  0.1014832  0.1093571  0.10278946 0.10239378 0.09866591\n",
      "  0.10023679 0.09597545 0.10055615 0.09679427]\n",
      " [0.09177482 0.10118843 0.10950777 0.10245935 0.10227175 0.0993179\n",
      "  0.10032352 0.09586875 0.10047148 0.09681622]\n",
      " [0.09184226 0.100997   0.10975925 0.10238057 0.10210912 0.09907272\n",
      "  0.10031794 0.09608835 0.10053651 0.09689628]\n",
      " [0.09168986 0.10047487 0.10995913 0.10250995 0.10205282 0.09888823\n",
      "  0.1003295  0.09612646 0.10056266 0.09740653]\n",
      " [0.09181356 0.10126575 0.1095022  0.10241375 0.1021845  0.09877786\n",
      "  0.10054488 0.095945   0.10059358 0.09695892]\n",
      " [0.09183767 0.10100328 0.10981981 0.1027455  0.10215387 0.0988507\n",
      "  0.10067319 0.09596526 0.10047979 0.09647094]\n",
      " [0.09195527 0.10124203 0.10932839 0.1024659  0.10187125 0.0991135\n",
      "  0.10036076 0.09627034 0.10044733 0.09694522]\n",
      " [0.09159805 0.10087337 0.11001096 0.10283647 0.10217177 0.09892522\n",
      "  0.09997127 0.09612626 0.10037933 0.0971073 ]\n",
      " [0.09176109 0.10072682 0.11002145 0.10243603 0.10189408 0.09931383\n",
      "  0.10037699 0.09600974 0.10022651 0.09723346]\n",
      " [0.09169197 0.10100395 0.10969377 0.10253111 0.10209253 0.09882004\n",
      "  0.10014491 0.09614605 0.10076323 0.09711243]\n",
      " [0.09184994 0.10128485 0.10911085 0.10292538 0.10244219 0.0988369\n",
      "  0.10014732 0.09578223 0.1004073  0.09721304]\n",
      " [0.09203709 0.10096227 0.10993327 0.10269092 0.10192043 0.09888309\n",
      "  0.10003925 0.09612521 0.10032838 0.0970801 ]\n",
      " [0.09151366 0.10118484 0.11011769 0.10238573 0.10214675 0.09923644\n",
      "  0.1003666  0.09604397 0.10038058 0.09662373]\n",
      " [0.09189268 0.10099764 0.10947071 0.10272007 0.10201909 0.09885558\n",
      "  0.10038232 0.0963084  0.10044224 0.09691126]\n",
      " [0.09170815 0.10104222 0.1098654  0.10248174 0.10179371 0.09901511\n",
      "  0.10029165 0.09633954 0.10041671 0.09704576]\n",
      " [0.09190961 0.10101847 0.10948062 0.10293902 0.10240792 0.09860082\n",
      "  0.10006495 0.09619388 0.10021303 0.09717168]\n",
      " [0.09171806 0.10066386 0.10961096 0.10282703 0.10238424 0.09884055\n",
      "  0.10052893 0.09576037 0.10047761 0.09718838]\n",
      " [0.09177032 0.10085614 0.10970047 0.10232959 0.10199554 0.09909482\n",
      "  0.10022381 0.09613654 0.1008122  0.09708057]\n",
      " [0.09138635 0.10111719 0.10984175 0.10256361 0.10224679 0.09918894\n",
      "  0.10017811 0.09591983 0.10059639 0.09696102]\n",
      " [0.09192254 0.1010241  0.10974895 0.10226684 0.10246628 0.09898124\n",
      "  0.10034551 0.09587673 0.10041741 0.0969504 ]\n",
      " [0.09174362 0.10088317 0.10995081 0.10271531 0.10231926 0.09892491\n",
      "  0.10012378 0.09605665 0.10048487 0.09679761]\n",
      " [0.09187964 0.10110011 0.10902903 0.10272073 0.10232812 0.09859052\n",
      "  0.10058763 0.09630271 0.10061047 0.09685104]\n",
      " [0.09189909 0.1008496  0.10972286 0.10275165 0.10185462 0.0990306\n",
      "  0.10023356 0.09623332 0.10043668 0.09698801]\n",
      " [0.09199205 0.1009994  0.10936041 0.10274919 0.10217034 0.09899068\n",
      "  0.10040634 0.09588631 0.10061193 0.09683335]\n",
      " [0.09175171 0.10105202 0.10959318 0.1029399  0.10217891 0.09870017\n",
      "  0.10011512 0.09603213 0.10062021 0.09701666]\n",
      " [0.09171634 0.10137861 0.10977544 0.10278181 0.10212107 0.09914797\n",
      "  0.1001642  0.09612251 0.10029732 0.09649474]\n",
      " [0.09183976 0.10117179 0.10909668 0.10260769 0.10206981 0.0988061\n",
      "  0.10055033 0.0961265  0.10058861 0.09714273]\n",
      " [0.09182747 0.1008729  0.10983377 0.10254641 0.10216179 0.09885719\n",
      "  0.10032273 0.09628442 0.1004743  0.096819  ]\n",
      " [0.09174791 0.10083647 0.10922138 0.10287667 0.10223421 0.09884383\n",
      "  0.10030614 0.09635107 0.10062655 0.09695575]\n",
      " [0.09188161 0.1010531  0.10983357 0.10237556 0.10211672 0.09900443\n",
      "  0.10041851 0.0960879  0.10041041 0.09681819]\n",
      " [0.09166937 0.10079142 0.109566   0.1030565  0.10218052 0.09880399\n",
      "  0.10019224 0.09613394 0.10064938 0.09695663]\n",
      " [0.09182335 0.10091297 0.1095131  0.10240878 0.10220802 0.09891003\n",
      "  0.10060318 0.09613926 0.10040487 0.09707646]\n",
      " [0.09169706 0.1013711  0.10973998 0.10254954 0.10204123 0.09890558\n",
      "  0.10051078 0.09587343 0.1003748  0.0969365 ]\n",
      " [0.09169544 0.10082988 0.10923049 0.10272428 0.10242324 0.09893638\n",
      "  0.1003805  0.09595692 0.10057843 0.09724444]\n",
      " [0.09165463 0.10161827 0.10890168 0.10275111 0.10257526 0.09896731\n",
      "  0.10043907 0.0956145  0.10059375 0.09688443]\n",
      " [0.09162215 0.10102503 0.10958674 0.1027239  0.10188579 0.09887193\n",
      "  0.10048418 0.09593947 0.10095354 0.09690728]\n",
      " [0.0919576  0.10087487 0.10911927 0.10276954 0.1023163  0.0992259\n",
      "  0.10038637 0.0958438  0.10044211 0.09706425]\n",
      " [0.09180277 0.10086814 0.1096137  0.10259481 0.10236263 0.09877978\n",
      "  0.10014416 0.09605109 0.10058052 0.09720241]\n",
      " [0.09155412 0.10106153 0.10940196 0.10259258 0.10232137 0.09905494\n",
      "  0.10029846 0.09643371 0.10054346 0.09673787]\n",
      " [0.09182923 0.10137382 0.10926818 0.10301838 0.10228421 0.09882774\n",
      "  0.10021182 0.09586272 0.10052359 0.09680031]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "print(x.shape)\n",
    "y = net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9104f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc746f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 학습 구현\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ead02c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[0;32m     16\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[1;32m---> 18\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     21\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad[key]\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\two_layer_net.py:48\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     45\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     47\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 48\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     50\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\..\\common\\gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m tmp_val \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[0;32m     42\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tmp_val) \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m---> 43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[0;32m     46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\two_layer_net.py:45\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 45\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     48\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\two_layer_net.py:33\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     31\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcross_entropy_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\바탕 화면\\deep_learning\\ch4\\..\\common\\functions.py:56\u001b[0m, in \u001b[0;36mcross_entropy_error\u001b[1;34m(y, t)\u001b[0m\n\u001b[0;32m     53\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)) \u001b[38;5;241m/\u001b[39m batch_size\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "lr = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    \n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = x_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= lr * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
